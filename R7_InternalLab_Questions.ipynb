{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8428fdfc-29e2-4873-fea1-3fc024f9de2c"
      },
      "source": [
        "print(x_train.shape[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e0ff714-178e-4052-aafe-240c04344ad0"
      },
      "source": [
        "print(x_test.shape[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "36f56bd6-a1f0-4fd3-c8ce-e395a43781c3"
      },
      "source": [
        "print(x_train.shape[1:])\n",
        "print(x_test[0].shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obMQEAwGPOHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "8f8e4d80-1b62-428e-89ad-92144a56d38a"
      },
      "source": [
        "pd.value_counts(y_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7    1000\n",
              "6    1000\n",
              "5    1000\n",
              "4    1000\n",
              "3    1000\n",
              "2    1000\n",
              "9    1000\n",
              "1    1000\n",
              "8    1000\n",
              "0    1000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgHSCXy3JjwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "f2342a63-ed02-4337-8a6c-86fc7cebeb36"
      },
      "source": [
        "pd.value_counts(y_train)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    6000\n",
              "8    6000\n",
              "7    6000\n",
              "6    6000\n",
              "5    6000\n",
              "4    6000\n",
              "3    6000\n",
              "2    6000\n",
              "1    6000\n",
              "0    6000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nv19MnvPiw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train,num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test,num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwKkHXn6QXO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b45cb84-7941-4202-958f-65a3994bff6b"
      },
      "source": [
        "x_train.dtype"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fUQpMHxJjwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GzWSVNhQvPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "200308df-3a9c-4098-ab4b-b917876361f7"
      },
      "source": [
        "x_train.dtype"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okwo_SB5JjwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
        "x_test = x_test.reshape(x_test.shape[0],28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i2Az_mqR46x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c6a17b7-f099-4ff6-96ee-5d17f7088c0b"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout,Reshape,Flatten\n",
        "from keras.layers import Convolution2D,MaxPooling2D\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "37da32e7-7979-48c1-99a8-709eda1a3668"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Convolution2D(32,3,3))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]\n",
        "\n",
        "model.fit(x_train, y_train, nb_epoch=10, batch_size=32,\n",
        "validation_data=(x_test, y_test), callbacks=callback_list, verbose=True)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 204s 3ms/step - loss: 0.3661 - acc: 0.8673 - val_loss: 0.3049 - val_acc: 0.8854\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 203s 3ms/step - loss: 0.2290 - acc: 0.9153 - val_loss: 0.2461 - val_acc: 0.9075\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 203s 3ms/step - loss: 0.1646 - acc: 0.9383 - val_loss: 0.2504 - val_acc: 0.9115\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 203s 3ms/step - loss: 0.1147 - acc: 0.9571 - val_loss: 0.2819 - val_acc: 0.9124\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0773 - acc: 0.9714 - val_loss: 0.3181 - val_acc: 0.9120\n",
            "Epoch 6/10\n",
            "32576/60000 [===============>..............] - ETA: 1:30 - loss: 0.0457 - acc: 0.9837"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-c34fd213f337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m model.fit(x_train, y_train, nb_epoch=10, batch_size=32,\n\u001b[0;32m---> 22\u001b[0;31m validation_data=(x_test, y_test), callbacks=callback_list, verbose=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "08d65248-59ca-47de-8c62-ca3d4cccf8d9"
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "model2.add(Convolution2D(32,3,3))\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128))\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "model2.add(Dense(10))\n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]\n",
        "\n",
        "model2.fit(x_train, y_train, nb_epoch=10, batch_size=32,\n",
        "validation_data=(x_test, y_test), callbacks=callback_list, verbose=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 146s 2ms/step - loss: 0.3932 - acc: 0.8589 - val_loss: 0.2933 - val_acc: 0.8949\n",
            "Epoch 2/10\n",
            " 6880/60000 [==>...........................] - ETA: 2:04 - loss: 0.2675 - acc: 0.9001"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-e5cd137d79ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m model2.fit(x_train, y_train, nb_epoch=10, batch_size=32,\n\u001b[0;32m---> 26\u001b[0;31m validation_data=(x_test, y_test), callbacks=callback_list, verbose=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbz4uHBuJjwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "outputId": "33911c21-c1d1-4547-ce0d-44d8f6204835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "d49d14fd-21bb-439b-c9a3-04c105cd4ce8"
      },
      "source": [
        "model4 = Sequential()\n",
        "\n",
        "model4.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "model4.add(Convolution2D(32, 3, 3))\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(128))\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Dense(10))\n",
        "model4.add(Activation('softmax'))\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model4.fit_generator(datagen.flow(x_train, y_train,\n",
        "                    batch_size=32),\n",
        "                    samples_per_epoch=x_train.shape[0],\n",
        "                    nb_epoch=10,\n",
        "                    validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=1875, epochs=10)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 160s 85ms/step - loss: 0.9553 - acc: 0.6469 - val_loss: 0.6135 - val_acc: 0.7704\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.7234 - acc: 0.7278 - val_loss: 0.5368 - val_acc: 0.8015\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.6652 - acc: 0.7506 - val_loss: 0.5440 - val_acc: 0.8088\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.6271 - acc: 0.7642 - val_loss: 0.4788 - val_acc: 0.8284\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.5998 - acc: 0.7750 - val_loss: 0.4744 - val_acc: 0.8296\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.5809 - acc: 0.7857 - val_loss: 0.4473 - val_acc: 0.8427\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.5645 - acc: 0.7894 - val_loss: 0.4354 - val_acc: 0.8421\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 158s 84ms/step - loss: 0.5487 - acc: 0.7972 - val_loss: 0.4341 - val_acc: 0.8452\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 159s 85ms/step - loss: 0.5394 - acc: 0.8003 - val_loss: 0.4516 - val_acc: 0.8401\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 160s 85ms/step - loss: 0.5311 - acc: 0.8046 - val_loss: 0.4133 - val_acc: 0.8514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4afd316fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1SrtBEPJjwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "207e21a1-920a-416a-e0e9-576058227118"
      },
      "source": [
        "model4.evaluate(x_test, y_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 622us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.41334342503547666, 0.8514]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwVWNQC2qZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "# Load/Prep the Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Pht1ggHuiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "317B7YFDawn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a90c542f-6a73-4a91-ebe9-f99692d217ef"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 32,32, 3).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 32, 32, 3).astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_train.shape[0])\n",
        "print(x_test.shape[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "60840626-5495-41e6-d3a7-7c6463224d91"
      },
      "source": [
        "gen = datagen.flow(x_train[:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABcCAYAAAB3AO7GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i91W50FOi_fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}