{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnSsH8sNOB6F",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Reset Default graph - Needed only for Jupyter notebook\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7K8pWsNQOB6X",
    "outputId": "297dd9f2-0289-4570-ebd3-9f6b2b1b9c79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'date', u'symbol', u'open', u'close', u'low', u'high', u'volume'], dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=['date','symbol'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "0bfb752f-a855-450a-a8af-348339d38643"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s",
    "outputId": "0f5afce1-e8a5-45f3-b2b5-3848be49c491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159108, 5)"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "datanew = data.iloc[0:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y2Z5ftmSfXLP",
    "outputId": "023372b4-cdd7-4d4b-fbd3-b0d34b2a2e9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datanew.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaZh-GI0feUF"
   },
   "outputs": [],
   "source": [
    "X = datanew.drop('close',axis = 1)\n",
    "Y = datanew['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHIGl4oalG_J"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eERs7g8olHOM"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "EMyzRHT128Bh",
    "outputId": "b606fea3-5c4a-443e-ec70-263d6cc551a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 4)\n",
      "(700,)\n",
      "(300, 4)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the graph in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xK0zBd1VOB64",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.Define input data placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDrYlWTuOB66"
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\n",
    "\n",
    "#Normalize the data\n",
    "x_n = tf.nn.l2_normalize(x,1)\n",
    "\n",
    "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H9_peAkGl7RZ",
    "outputId": "0e289f67-4398-4893-bf87-7584dc88057d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(4)])"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kch2TgWGtYnx",
    "outputId": "821dd5f5-67b5-4b90-cb30-a8696d7c450b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None)])"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\n",
    "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "y = tf.add(tf.matmul(x_n,W),b,name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8keVNjLBsw46",
    "outputId": "5a68a0c8-8459-4e1a-aa21-e05c46601809"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output_7:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 145,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OqamvGbrsf3_",
    "outputId": "e8c6e0f4-c493-43ee-9c4a-88bca0e1f17e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Loss_7:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5.GradientDescent Optimizer to minimize Loss [GradientDescentOptimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Execute the Graph for 100 epochs and observe the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVvgj7eQOB7f"
   },
   "outputs": [],
   "source": [
    "#Lets start graph Execution\n",
    "sess = tf.Session()\n",
    "\n",
    "# variables need to be initialized before we can use them\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#how many times data need to be shown to model\n",
    "training_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "9smwOW-1OB7k",
    "outputId": "144621c6-2201-4524-963e-86e65cc504fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training loss at step: ', 0, ' is ', 7695.2324)\n",
      "('Testing loss at step: ', 0, ' is ', 7211.8906)\n",
      "('Training loss at step: ', 10, ' is ', 3428.4355)\n",
      "('Testing loss at step: ', 10, ' is ', 3395.1965)\n",
      "('Training loss at step: ', 20, ' is ', 3425.143)\n",
      "('Testing loss at step: ', 20, ' is ', 3352.7542)\n",
      "('Training loss at step: ', 30, ' is ', 3426.7341)\n",
      "('Testing loss at step: ', 30, ' is ', 3350.9978)\n",
      "('Training loss at step: ', 40, ' is ', 3426.84)\n",
      "('Testing loss at step: ', 40, ' is ', 3350.8281)\n",
      "('Training loss at step: ', 50, ' is ', 3426.8342)\n",
      "('Testing loss at step: ', 50, ' is ', 3350.8152)\n",
      "('Training loss at step: ', 60, ' is ', 3426.835)\n",
      "('Testing loss at step: ', 60, ' is ', 3350.8167)\n",
      "('Training loss at step: ', 70, ' is ', 3426.835)\n",
      "('Testing loss at step: ', 70, ' is ', 3350.8167)\n",
      "('Training loss at step: ', 80, ' is ', 3426.835)\n",
      "('Testing loss at step: ', 80, ' is ', 3350.8167)\n",
      "('Training loss at step: ', 90, ' is ', 3426.835)\n",
      "('Testing loss at step: ', 90, ' is ', 3350.8167)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "            \n",
    "    #Calculate train_op and loss\n",
    "    _, train_loss = sess.run([train_op,loss],feed_dict={x:X_train, y_:y_train})\n",
    "    \n",
    "    #Calculate test_op and loss\n",
    "    _, test_loss = sess.run([train_op,loss],feed_dict={x:X_test, y_:y_test})\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print ('Training loss at step: ', epoch, ' is ', train_loss)\n",
    "        print ('Testing loss at step: ', epoch, ' is ', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JuLI6bSOB7n"
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b\n",
    "\n",
    "Hint: Use sess.run(W) to get W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r",
    "outputId": "3df21044-9afc-4a53-db47-16800133b93a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.0659204e-03],\n",
       "       [3.0405428e-03],\n",
       "       [3.0881043e-03],\n",
       "       [3.3927490e+01]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vhDtOv5UOB7x",
    "outputId": "d5a98215-5b2a-4136-ca61-fd0cdf39da62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(1)])"
      ]
     },
     "execution_count": 153,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4cB3n0NRo7aZ",
    "outputId": "01d19acc-9958-4652-86ea-65f550368014"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RefVariable.value of <tf.Variable 'Weights_6:0' shape=(4, 1) dtype=float32_ref>>"
      ]
     },
     "execution_count": 154,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kqFcDAkfo7eY",
    "outputId": "bb1a5157-ba31-4d28-ef87-150f31969e62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RefVariable.value of <tf.Variable 'Bias_6:0' shape=(1,) dtype=float32_ref>>"
      ]
     },
     "execution_count": 155,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZqKUEFsOB71"
   },
   "source": [
    "### Find the Absolute mean square loss difference between training and testing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "97t-grQgOB71",
    "outputId": "8322adfe-5ace-4bbf-e334-1ce516e771ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5778.7837"
      ]
     },
     "execution_count": 156,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(train_loss- test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjOInjUROB75"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZUAjZ5oOB78"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "### Linear Classification using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GoNTWXAOB8C",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
    "#### Use Mean square error as loss function and sgd as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpeL5rCTOB8D"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfvOjg0a6J92"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkKadrC72Eza"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1M-1yEn8h63"
   },
   "outputs": [],
   "source": [
    "X_train1 = np.array(X_train)\n",
    "y_train1 = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt-HYFMEOB8G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3410
    },
    "colab_type": "code",
    "id": "66JGJt7GOB8H",
    "outputId": "7b66dd0f-af74-4c26-fad5-39581a388e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/100\n",
      "700/700 [==============================] - 0s 257us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 2/100\n",
      "700/700 [==============================] - 0s 6us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 3/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 4/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 5/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 6/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 7/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 8/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 9/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 10/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 11/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 12/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 13/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 14/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 15/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 16/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 17/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 18/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 19/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 20/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 21/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 22/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 23/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 24/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 25/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 26/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 27/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 28/100\n",
      "700/700 [==============================] - 0s 5us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 29/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 30/100\n",
      "700/700 [==============================] - 0s 5us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 31/100\n",
      "700/700 [==============================] - 0s 7us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 32/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6777 - val_loss: 8110.2100\n",
      "Epoch 33/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 34/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 35/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 36/100\n",
      "700/700 [==============================] - 0s 5us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 37/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 38/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 39/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 40/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 41/100\n",
      "700/700 [==============================] - 0s 5us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 42/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 43/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 44/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 45/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 46/100\n",
      "700/700 [==============================] - 0s 5us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 47/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 48/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 49/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 50/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 51/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 52/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 53/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 54/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 55/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 56/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 57/100\n",
      "700/700 [==============================] - 0s 9us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 58/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 59/100\n",
      "700/700 [==============================] - 0s 6us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 60/100\n",
      "700/700 [==============================] - 0s 5us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 61/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 62/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6777 - val_loss: 8110.2100\n",
      "Epoch 63/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 64/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 65/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 66/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 67/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 68/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 69/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 70/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 71/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 72/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 73/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 74/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 75/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 76/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 77/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 78/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 79/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 80/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 81/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 82/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 83/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 84/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 85/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 86/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 87/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 88/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 89/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 90/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 91/100\n",
      "700/700 [==============================] - 0s 4us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 92/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 93/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 94/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 95/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 96/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 97/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n",
      "Epoch 98/100\n",
      "700/700 [==============================] - 0s 2us/step - loss: 7565.6772 - val_loss: 8110.2100\n",
      "Epoch 99/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6758 - val_loss: 8110.2100\n",
      "Epoch 100/100\n",
      "700/700 [==============================] - 0s 3us/step - loss: 7565.6763 - val_loss: 8110.2100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5412cfd690>"
      ]
     },
     "execution_count": 200,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train1, y_train1, \n",
    "          validation_data=(X_test, y_test), \n",
    "          epochs=100,\n",
    "          batch_size = X_train1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPonxP25dcz5"
   },
   "source": [
    "### Classification using Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdJsB0J9dcz8"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JUTdkrNdcz8"
   },
   "outputs": [],
   "source": [
    "df_iris = pd.read_csv('Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "QNVn9FEM1RmE",
    "outputId": "7142bc7a-029b-4f3b-97c0-6535df80065a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length (in cm)</th>\n",
       "      <th>Sepal Width in (cm)</th>\n",
       "      <th>Petal length (in cm)</th>\n",
       "      <th>Petal width (in cm)</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal Length (in cm)  Sepal Width in (cm)  Petal length (in cm)  \\\n",
       "0                   5.1                  3.5                   1.4   \n",
       "1                   4.9                  3.0                   1.4   \n",
       "2                   4.7                  3.2                   1.3   \n",
       "3                   4.6                  3.1                   1.5   \n",
       "4                   5.0                  3.6                   1.4   \n",
       "\n",
       "   Petal width (in cm)        Class  \n",
       "0                  0.2  Iris-setosa  \n",
       "1                  0.2  Iris-setosa  \n",
       "2                  0.2  Iris-setosa  \n",
       "3                  0.2  Iris-setosa  \n",
       "4                  0.2  Iris-setosa  "
      ]
     },
     "execution_count": 235,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjYwaH2kdcz_"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIifIeQPdcz_"
   },
   "outputs": [],
   "source": [
    "X_iris = df_iris.drop('Class',axis=1)\n",
    "Y_iris = df_iris['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ZDvaqo5P1qds",
    "outputId": "1cb4abcf-fb56-42e2-b95a-7e472d91290c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(X_iris.shape)\n",
    "print(Y_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzOCfGttdc0C"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efHC0Fzfdc0C"
   },
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lR9V1LH3wu3"
   },
   "outputs": [],
   "source": [
    "#le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRffp7Ft3wxp"
   },
   "outputs": [],
   "source": [
    "#Y_iris = le.fit_transform(Y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6jefajpLFev"
   },
   "outputs": [],
   "source": [
    "Y_iris = pd.get_dummies(Y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1882
    },
    "colab_type": "code",
    "id": "lDk75qMO3-oA",
    "outputId": "1d1f7c6c-248f-4ecf-f1c8-60ba999bac10"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0              1                0               0\n",
       "1              1                0               0\n",
       "2              1                0               0\n",
       "3              1                0               0\n",
       "4              1                0               0\n",
       "5              1                0               0\n",
       "6              1                0               0\n",
       "7              1                0               0\n",
       "8              1                0               0\n",
       "9              1                0               0\n",
       "10             1                0               0\n",
       "11             1                0               0\n",
       "12             1                0               0\n",
       "13             1                0               0\n",
       "14             1                0               0\n",
       "15             1                0               0\n",
       "16             1                0               0\n",
       "17             1                0               0\n",
       "18             1                0               0\n",
       "19             1                0               0\n",
       "20             1                0               0\n",
       "21             1                0               0\n",
       "22             1                0               0\n",
       "23             1                0               0\n",
       "24             1                0               0\n",
       "25             1                0               0\n",
       "26             1                0               0\n",
       "27             1                0               0\n",
       "28             1                0               0\n",
       "29             1                0               0\n",
       "..           ...              ...             ...\n",
       "120            0                0               1\n",
       "121            0                0               1\n",
       "122            0                0               1\n",
       "123            0                0               1\n",
       "124            0                0               1\n",
       "125            0                0               1\n",
       "126            0                0               1\n",
       "127            0                0               1\n",
       "128            0                0               1\n",
       "129            0                0               1\n",
       "130            0                0               1\n",
       "131            0                0               1\n",
       "132            0                0               1\n",
       "133            0                0               1\n",
       "134            0                0               1\n",
       "135            0                0               1\n",
       "136            0                0               1\n",
       "137            0                0               1\n",
       "138            0                0               1\n",
       "139            0                0               1\n",
       "140            0                0               1\n",
       "141            0                0               1\n",
       "142            0                0               1\n",
       "143            0                0               1\n",
       "144            0                0               1\n",
       "145            0                0               1\n",
       "146            0                0               1\n",
       "147            0                0               1\n",
       "148            0                0               1\n",
       "149            0                0               1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_l1SEBOxdc0E"
   },
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wngxwFDEdc0F"
   },
   "outputs": [],
   "source": [
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, Y_iris,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zny3fcc9ldy"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "10hSZDpa9mUS",
    "outputId": "cd1d6abd-44fa-419f-b948-f8d3314586dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4)\n",
      "(45, 4)\n",
      "(105, 3)\n",
      "(45, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_iris.shape)\n",
    "print(X_test_iris.shape)\n",
    "print(y_train_iris.shape)\n",
    "print(y_test_iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjnl8dwfdc0G"
   },
   "source": [
    "### Model\n",
    "Build the model with following layers: <br>\n",
    "1. First dense layer with 10 neurons with input shape 4 (according to the feature set) <br>\n",
    "2. Second Dense layer with 8 neurons <br>\n",
    "3. Output layer with 3 neurons with softmax activation (output layer, 3 neurons as we have 3 classes) <br>\n",
    "4. Use SGD and categorical_crossentropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CumfFZnu_ojZ"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "#model = tf.keras.models.Sequential()\n",
    "\n",
    "model_iris = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(8),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#Comile the model\n",
    "model_iris.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1379
    },
    "colab_type": "code",
    "id": "nVTgvdbLImVW",
    "outputId": "a1734798-5f6e-48dd-e92c-03f0aec79cd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "40             1                0               0\n",
       "12             1                0               0\n",
       "10             1                0               0\n",
       "35             1                0               0\n",
       "57             0                1               0\n",
       "105            0                0               1\n",
       "113            0                0               1\n",
       "131            0                0               1\n",
       "111            0                0               1\n",
       "2              1                0               0\n",
       "139            0                0               1\n",
       "104            0                0               1\n",
       "144            0                0               1\n",
       "68             0                1               0\n",
       "80             0                1               0\n",
       "127            0                0               1\n",
       "132            0                0               1\n",
       "101            0                0               1\n",
       "31             1                0               0\n",
       "78             0                1               0\n",
       "9              1                0               0\n",
       "66             0                1               0\n",
       "95             0                1               0\n",
       "98             0                1               0\n",
       "36             1                0               0\n",
       "137            0                0               1\n",
       "146            0                0               1\n",
       "29             1                0               0\n",
       "115            0                0               1\n",
       "42             1                0               0\n",
       "51             0                1               0\n",
       "4              1                0               0\n",
       "84             0                1               0\n",
       "106            0                0               1\n",
       "93             0                1               0\n",
       "119            0                0               1\n",
       "76             0                1               0\n",
       "65             0                1               0\n",
       "72             0                1               0\n",
       "32             1                0               0\n",
       "109            0                0               1\n",
       "58             0                1               0\n",
       "16             1                0               0\n",
       "63             0                1               0\n",
       "81             0                1               0"
      ]
     },
     "execution_count": 245,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "681u6xRxdc0K"
   },
   "source": [
    "### Fitting the model and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3410
    },
    "colab_type": "code",
    "id": "yuMg0Fw6dc0K",
    "outputId": "2be44d81-789c-46de-9ab8-22e267e28db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 2.2621 - acc: 0.3238 - val_loss: 1.4079 - val_acc: 0.2444\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 48us/step - loss: 1.5486 - acc: 0.2381 - val_loss: 1.0735 - val_acc: 0.0444\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 44us/step - loss: 1.1321 - acc: 0.0571 - val_loss: 0.9073 - val_acc: 0.0889\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 41us/step - loss: 0.9114 - acc: 0.0667 - val_loss: 0.8705 - val_acc: 0.3333\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 30us/step - loss: 0.8517 - acc: 0.4095 - val_loss: 0.8520 - val_acc: 0.3333\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.8289 - acc: 0.4190 - val_loss: 0.8335 - val_acc: 0.3778\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.8090 - acc: 0.4286 - val_loss: 0.8167 - val_acc: 0.3778\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 42us/step - loss: 0.7910 - acc: 0.4571 - val_loss: 0.8013 - val_acc: 0.3778\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.7745 - acc: 0.4667 - val_loss: 0.7871 - val_acc: 0.3778\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.7593 - acc: 0.4762 - val_loss: 0.7740 - val_acc: 0.4000\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 40us/step - loss: 0.7454 - acc: 0.5048 - val_loss: 0.7619 - val_acc: 0.4000\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 35us/step - loss: 0.7324 - acc: 0.5048 - val_loss: 0.7506 - val_acc: 0.4000\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 36us/step - loss: 0.7204 - acc: 0.5143 - val_loss: 0.7402 - val_acc: 0.4000\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 42us/step - loss: 0.7093 - acc: 0.5143 - val_loss: 0.7304 - val_acc: 0.4000\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.6989 - acc: 0.5238 - val_loss: 0.7212 - val_acc: 0.4222\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 31us/step - loss: 0.6891 - acc: 0.5333 - val_loss: 0.7126 - val_acc: 0.4444\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 16us/step - loss: 0.6800 - acc: 0.5333 - val_loss: 0.7045 - val_acc: 0.4444\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 15us/step - loss: 0.6714 - acc: 0.5333 - val_loss: 0.6969 - val_acc: 0.4444\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.6633 - acc: 0.5333 - val_loss: 0.6896 - val_acc: 0.4444\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.6557 - acc: 0.5429 - val_loss: 0.6828 - val_acc: 0.4444\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.6485 - acc: 0.5429 - val_loss: 0.6763 - val_acc: 0.4444\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.6416 - acc: 0.5429 - val_loss: 0.6701 - val_acc: 0.4667\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.6351 - acc: 0.5619 - val_loss: 0.6642 - val_acc: 0.4889\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.6289 - acc: 0.5714 - val_loss: 0.6586 - val_acc: 0.4889\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.6230 - acc: 0.5714 - val_loss: 0.6532 - val_acc: 0.5333\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.6174 - acc: 0.5905 - val_loss: 0.6480 - val_acc: 0.5556\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.6120 - acc: 0.5905 - val_loss: 0.6430 - val_acc: 0.5556\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 18us/step - loss: 0.6068 - acc: 0.6000 - val_loss: 0.6383 - val_acc: 0.5556\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.6018 - acc: 0.6000 - val_loss: 0.6337 - val_acc: 0.5556\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 15us/step - loss: 0.5970 - acc: 0.6000 - val_loss: 0.6293 - val_acc: 0.5556\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.5924 - acc: 0.6095 - val_loss: 0.6250 - val_acc: 0.5778\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 18us/step - loss: 0.5880 - acc: 0.6190 - val_loss: 0.6209 - val_acc: 0.5778\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.5838 - acc: 0.6190 - val_loss: 0.6169 - val_acc: 0.5778\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 14us/step - loss: 0.5796 - acc: 0.6190 - val_loss: 0.6131 - val_acc: 0.5778\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 29us/step - loss: 0.5757 - acc: 0.6286 - val_loss: 0.6093 - val_acc: 0.6000\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 16us/step - loss: 0.5718 - acc: 0.6381 - val_loss: 0.6057 - val_acc: 0.6222\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.5681 - acc: 0.6381 - val_loss: 0.6022 - val_acc: 0.6222\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 16us/step - loss: 0.5644 - acc: 0.6381 - val_loss: 0.5987 - val_acc: 0.6222\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.5609 - acc: 0.6476 - val_loss: 0.5954 - val_acc: 0.6444\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.5575 - acc: 0.6476 - val_loss: 0.5921 - val_acc: 0.6444\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.5542 - acc: 0.6476 - val_loss: 0.5890 - val_acc: 0.6444\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 30us/step - loss: 0.5509 - acc: 0.6476 - val_loss: 0.5859 - val_acc: 0.6444\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 32us/step - loss: 0.5478 - acc: 0.6476 - val_loss: 0.5828 - val_acc: 0.6444\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 30us/step - loss: 0.5447 - acc: 0.6571 - val_loss: 0.5799 - val_acc: 0.6444\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.5417 - acc: 0.6571 - val_loss: 0.5770 - val_acc: 0.6444\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 20us/step - loss: 0.5388 - acc: 0.6571 - val_loss: 0.5742 - val_acc: 0.6444\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 34us/step - loss: 0.5359 - acc: 0.6667 - val_loss: 0.5714 - val_acc: 0.6444\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 28us/step - loss: 0.5332 - acc: 0.6667 - val_loss: 0.5687 - val_acc: 0.6444\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 31us/step - loss: 0.5304 - acc: 0.6667 - val_loss: 0.5660 - val_acc: 0.6444\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 28us/step - loss: 0.5277 - acc: 0.6667 - val_loss: 0.5634 - val_acc: 0.6444\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.5251 - acc: 0.6762 - val_loss: 0.5609 - val_acc: 0.6444\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.5226 - acc: 0.6762 - val_loss: 0.5584 - val_acc: 0.6444\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.5200 - acc: 0.6762 - val_loss: 0.5559 - val_acc: 0.6444\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 30us/step - loss: 0.5176 - acc: 0.6762 - val_loss: 0.5535 - val_acc: 0.6444\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 32us/step - loss: 0.5151 - acc: 0.6762 - val_loss: 0.5511 - val_acc: 0.6444\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 25us/step - loss: 0.5128 - acc: 0.6762 - val_loss: 0.5487 - val_acc: 0.6444\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.5104 - acc: 0.6762 - val_loss: 0.5464 - val_acc: 0.6444\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 36us/step - loss: 0.5081 - acc: 0.6762 - val_loss: 0.5441 - val_acc: 0.6667\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.5059 - acc: 0.6762 - val_loss: 0.5419 - val_acc: 0.6667\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 26us/step - loss: 0.5036 - acc: 0.6762 - val_loss: 0.5397 - val_acc: 0.6667\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.5014 - acc: 0.6762 - val_loss: 0.5375 - val_acc: 0.6667\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.4993 - acc: 0.6762 - val_loss: 0.5354 - val_acc: 0.6667\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.4972 - acc: 0.6762 - val_loss: 0.5332 - val_acc: 0.6667\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 27us/step - loss: 0.4951 - acc: 0.6762 - val_loss: 0.5311 - val_acc: 0.6667\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 27us/step - loss: 0.4930 - acc: 0.6762 - val_loss: 0.5291 - val_acc: 0.6667\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 36us/step - loss: 0.4910 - acc: 0.6762 - val_loss: 0.5270 - val_acc: 0.6667\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 26us/step - loss: 0.4890 - acc: 0.6762 - val_loss: 0.5250 - val_acc: 0.6667\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 38us/step - loss: 0.4870 - acc: 0.6762 - val_loss: 0.5230 - val_acc: 0.6667\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 53us/step - loss: 0.4850 - acc: 0.6857 - val_loss: 0.5210 - val_acc: 0.6667\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 41us/step - loss: 0.4831 - acc: 0.6857 - val_loss: 0.5191 - val_acc: 0.6667\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 31us/step - loss: 0.4812 - acc: 0.6857 - val_loss: 0.5172 - val_acc: 0.6667\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 32us/step - loss: 0.4793 - acc: 0.6857 - val_loss: 0.5153 - val_acc: 0.6667\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 25us/step - loss: 0.4775 - acc: 0.6952 - val_loss: 0.5134 - val_acc: 0.6667\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.4756 - acc: 0.6952 - val_loss: 0.5115 - val_acc: 0.6667\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 28us/step - loss: 0.4738 - acc: 0.6952 - val_loss: 0.5096 - val_acc: 0.6667\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 27us/step - loss: 0.4720 - acc: 0.6952 - val_loss: 0.5078 - val_acc: 0.6889\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 26us/step - loss: 0.4702 - acc: 0.6952 - val_loss: 0.5060 - val_acc: 0.7111\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.4685 - acc: 0.6952 - val_loss: 0.5042 - val_acc: 0.7111\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.4667 - acc: 0.7048 - val_loss: 0.5024 - val_acc: 0.7111\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.4650 - acc: 0.7048 - val_loss: 0.5006 - val_acc: 0.7111\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.4633 - acc: 0.7048 - val_loss: 0.4989 - val_acc: 0.7333\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 19us/step - loss: 0.4616 - acc: 0.7048 - val_loss: 0.4971 - val_acc: 0.7333\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.4599 - acc: 0.7048 - val_loss: 0.4954 - val_acc: 0.7333\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 18us/step - loss: 0.4582 - acc: 0.7143 - val_loss: 0.4937 - val_acc: 0.7333\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.4566 - acc: 0.7143 - val_loss: 0.4920 - val_acc: 0.7333\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 22us/step - loss: 0.4550 - acc: 0.7238 - val_loss: 0.4903 - val_acc: 0.7556\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.7238 - val_loss: 0.4887 - val_acc: 0.7556\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.7333 - val_loss: 0.4870 - val_acc: 0.7556\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.4502 - acc: 0.7333 - val_loss: 0.4854 - val_acc: 0.7556\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 25us/step - loss: 0.4486 - acc: 0.7429 - val_loss: 0.4837 - val_acc: 0.7556\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 26us/step - loss: 0.4470 - acc: 0.7429 - val_loss: 0.4821 - val_acc: 0.7778\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 29us/step - loss: 0.4455 - acc: 0.7619 - val_loss: 0.4805 - val_acc: 0.8000\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 31us/step - loss: 0.4439 - acc: 0.7714 - val_loss: 0.4789 - val_acc: 0.8000\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 21us/step - loss: 0.4424 - acc: 0.7810 - val_loss: 0.4773 - val_acc: 0.8000\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 28us/step - loss: 0.4409 - acc: 0.7905 - val_loss: 0.4757 - val_acc: 0.8000\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 26us/step - loss: 0.4394 - acc: 0.8000 - val_loss: 0.4741 - val_acc: 0.8000\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 23us/step - loss: 0.4379 - acc: 0.8190 - val_loss: 0.4726 - val_acc: 0.8000\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 30us/step - loss: 0.4364 - acc: 0.8286 - val_loss: 0.4710 - val_acc: 0.8000\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 24us/step - loss: 0.4349 - acc: 0.8286 - val_loss: 0.4695 - val_acc: 0.8222\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 28us/step - loss: 0.4334 - acc: 0.8381 - val_loss: 0.4679 - val_acc: 0.8444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f541215b750>"
      ]
     },
     "execution_count": 246,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_iris.fit(X_train_iris, y_train_iris, \n",
    "          validation_data=(X_test_iris, y_test_iris), \n",
    "          epochs=100,\n",
    "          batch_size = X_train_iris.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otefnmXedc0O"
   },
   "source": [
    "### Report Accuracy of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZJ60X_8LqLR"
   },
   "outputs": [],
   "source": [
    "Predicted = model_iris.predict(X_test_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "idCdQ3m4LwW1",
    "outputId": "00c33d47-32db-4ac6-b5c7-1c43143ffd11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 3)\n",
      "(45, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Predicted.shape)\n",
    "print(y_test_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "z8Tgh-cQdc0P",
    "outputId": "1c7a3ff7-111d-4b8f-fe76-979f5b2e8425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 141us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4679404934247335, 0.8444444497426351]"
      ]
     },
     "execution_count": 255,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy = model_iris.evaluate(X_test_iris,y_test_iris)\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTgsojF6fKu4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YJRBuqXhOB7_"
   ],
   "name": "R6_InternalLab_AIML.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
